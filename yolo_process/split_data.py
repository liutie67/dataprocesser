"""
Step 3

YOLOæ•°æ®é›†åˆ†å‰²æ¨¡å—

åŠŸèƒ½ï¼šå°†YOLOæ ¼å¼çš„æ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
æ”¯æŒå¤šçº¿ç¨‹æ–‡ä»¶æ“ä½œï¼Œæé«˜å¤§æ•°æ®é›†å¤„ç†æ•ˆç‡

ä½œè€…ï¼šLIU Tie
ç‰ˆæœ¬ï¼š1.0
æ—¥æœŸï¼š2025-11-14
"""

import os
import shutil
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm


def split_yolo_dataset(path, val_ratio=0.2, seed=42, max_workers=8):
    """
    åˆ†å‰²YOLOæ ¼å¼çš„æ•°æ®é›†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†

    è¯¥å‡½æ•°ä¼šï¼š
    1. è¯»å–æŒ‡å®šè·¯å¾„ä¸‹çš„imageså’Œlabelsæ–‡ä»¶å¤¹
    2. æŒ‰æ¯”ä¾‹éšæœºåˆ†å‰²æ•°æ®é›†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
    3. åˆ›å»ºtrainå’Œvalæ–‡ä»¶å¤¹ç»“æ„
    4. ä½¿ç”¨å¤šçº¿ç¨‹ç§»åŠ¨å¯¹åº”çš„å›¾ç‰‡å’Œæ ‡ç­¾æ–‡ä»¶
    5. æ¸…ç†åŸå§‹çš„imageså’Œlabelsç©ºæ–‡ä»¶å¤¹

    Args:
        path (str): æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„ï¼ŒåŒ…å«imageså’Œlabelsæ–‡ä»¶å¤¹
        val_ratio (float): éªŒè¯é›†æ¯”ä¾‹ï¼ŒèŒƒå›´0-1ï¼Œé»˜è®¤0.2ï¼ˆ20%ï¼‰
        seed (int): éšæœºç§å­ï¼Œç”¨äºä¿è¯å¯é‡å¤æ€§ï¼Œé»˜è®¤42
        max_workers (int): æœ€å¤§çº¿ç¨‹æ•°ï¼Œç”¨äºå¹¶è¡Œæ–‡ä»¶æ“ä½œï¼Œé»˜è®¤8

    Returns:
        tuple: è¿”å›ä¸¤ä¸ªåˆ—è¡¨ (train_names, val_names)
               - train_names: è®­ç»ƒé›†æ–‡ä»¶ååˆ—è¡¨ï¼ˆä¸å«æ‰©å±•åï¼‰
               - val_names: éªŒè¯é›†æ–‡ä»¶ååˆ—è¡¨ï¼ˆä¸å«æ‰©å±•åï¼‰

    Raises:
        FileNotFoundError: å½“æŒ‡å®šçš„è·¯å¾„æˆ–images/labelsæ–‡ä»¶å¤¹ä¸å­˜åœ¨æ—¶
        ValueError: å½“æ²¡æœ‰æ‰¾åˆ°å›¾åƒæ–‡ä»¶æˆ–val_ratioè¶…å‡ºèŒƒå›´æ—¶

    Example:
        >>> # åŸºæœ¬ç”¨æ³•ï¼šä½¿ç”¨é»˜è®¤å‚æ•°
        >>> train_files, val_files = split_yolo_dataset('./dataset')
        >>>
        >>> # è‡ªå®šä¹‰éªŒè¯é›†æ¯”ä¾‹å’Œéšæœºç§å­
        >>> split_yolo_dataset('./dataset', val_ratio=0.3, seed=123)
        >>>
        >>> # ä½¿ç”¨æ›´å¤šçº¿ç¨‹åŠ é€Ÿå¤„ç†
        >>> split_yolo_dataset('./dataset', max_workers=16)
        >>>
        >>> # å¤„ç†ç‰¹å®šæ•°æ®é›†
        >>> split_yolo_dataset('./fire-smoke/combined-15000')
    """
    # å‚æ•°éªŒè¯
    if not 0 < val_ratio < 1:
        raise ValueError(f"val_ratioå¿…é¡»åœ¨0å’Œ1ä¹‹é—´ï¼Œå½“å‰å€¼: {val_ratio}")

    if max_workers < 1:
        raise ValueError(f"max_workerså¿…é¡»å¤§äº0ï¼Œå½“å‰å€¼: {max_workers}")

    # è®¾ç½®éšæœºç§å­ä¿è¯å¯é‡å¤æ€§
    random.seed(seed)

    # å®šä¹‰è·¯å¾„
    images_dir = os.path.join(path, 'images')
    labels_dir = os.path.join(path, 'labels')
    train_dir = os.path.join(path, 'train')
    val_dir = os.path.join(path, 'val')

    # éªŒè¯æºæ–‡ä»¶å¤¹å­˜åœ¨
    if not os.path.exists(images_dir):
        raise FileNotFoundError(f"å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {images_dir}")
    if not os.path.exists(labels_dir):
        raise FileNotFoundError(f"æ ‡ç­¾ç›®å½•ä¸å­˜åœ¨: {labels_dir}")

    print(f"ğŸ” å¼€å§‹å¤„ç†æ•°æ®é›†: {path}")
    print(f"ğŸ“ æºå›¾ç‰‡ç›®å½•: {images_dir}")
    print(f"ğŸ“ æºæ ‡ç­¾ç›®å½•: {labels_dir}")

    # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶å
    image_files = [f for f in os.listdir(images_dir)
                   if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

    if not image_files:
        raise ValueError(f"åœ¨å›¾ç‰‡ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•å›¾åƒæ–‡ä»¶: {images_dir}")

    # è·å–åŸºæœ¬æ–‡ä»¶åå’Œæ‰©å±•å
    base_names = [os.path.splitext(f)[0] for f in image_files]
    image_ext = os.path.splitext(image_files[0])[1]  # å‡è®¾æ‰€æœ‰å›¾ç‰‡æ‰©å±•åç›¸åŒ

    print(f"ğŸ“Š æ‰¾åˆ° {len(base_names)} ä¸ªå›¾åƒæ–‡ä»¶")

    # éšæœºæ‰“ä¹±å¹¶åˆ†å‰²æ•°æ®é›†
    random.shuffle(base_names)
    split_idx = int(len(base_names) * (1 - val_ratio))
    train_names = base_names[:split_idx]
    val_names = base_names[split_idx:]

    print(f"ğŸ“‹ æ•°æ®é›†åˆ†å‰²:")
    print(f"   - è®­ç»ƒé›†: {len(train_names)} ä¸ªæ ·æœ¬ ({len(train_names) / len(base_names) * 100:.1f}%)")
    print(f"   - éªŒè¯é›†: {len(val_names)} ä¸ªæ ·æœ¬ ({len(val_names) / len(base_names) * 100:.1f}%)")

    # åˆ›å»ºç›®æ ‡æ–‡ä»¶å¤¹ç»“æ„
    train_images_dir = os.path.join(train_dir, 'images')
    train_labels_dir = os.path.join(train_dir, 'labels')
    val_images_dir = os.path.join(val_dir, 'images')
    val_labels_dir = os.path.join(val_dir, 'labels')

    for dir_path in [train_images_dir, train_labels_dir, val_images_dir, val_labels_dir]:
        os.makedirs(dir_path, exist_ok=True)

    print("ğŸ“ åˆ›å»ºç›®æ ‡ç›®å½•ç»“æ„å®Œæˆ")

    def process_files(names, dest_dir, dataset_type):
        """
        å¤„ç†æ–‡ä»¶ç§»åŠ¨çš„å¤šçº¿ç¨‹å‡½æ•°

        Args:
            names: æ–‡ä»¶ååˆ—è¡¨ï¼ˆä¸å«æ‰©å±•åï¼‰
            dest_dir: ç›®æ ‡ç›®å½•
            dataset_type: æ•°æ®é›†ç±»å‹ï¼ˆ'train' æˆ– 'val'ï¼‰
        """
        file_pairs = []
        missing_labels = 0

        for name in names:
            # å›¾åƒæ–‡ä»¶è·¯å¾„
            src_img = os.path.join(images_dir, f"{name}{image_ext}")
            dst_img = os.path.join(dest_dir, 'images', f"{name}{image_ext}")
            file_pairs.append((src_img, dst_img))

            # æ ‡ç­¾æ–‡ä»¶è·¯å¾„
            src_label = os.path.join(labels_dir, f"{name}.txt")
            if os.path.exists(src_label):
                dst_label = os.path.join(dest_dir, 'labels', f"{name}.txt")
                file_pairs.append((src_label, dst_label))
            else:
                missing_labels += 1

        if missing_labels > 0:
            print(f"âš ï¸  è­¦å‘Š: {dataset_type}é›†ä¸­æœ‰ {missing_labels} ä¸ªå›¾åƒæ²¡æœ‰å¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶")

        # å¤šçº¿ç¨‹ç§»åŠ¨æ–‡ä»¶
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(shutil.move, src, dst) for src, dst in file_pairs]

            # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºç§»åŠ¨è¿›åº¦
            for future in tqdm(as_completed(futures), total=len(futures),
                               desc=f"ğŸšš ç§»åŠ¨ {dataset_type} é›†æ–‡ä»¶"):
                try:
                    future.result()  # è·å–ç»“æœï¼Œå¦‚æœ‰å¼‚å¸¸ä¼šæŠ›å‡º
                except Exception as e:
                    print(f"âŒ æ–‡ä»¶ç§»åŠ¨å¤±è´¥: {e}")

        return len(file_pairs)

    # å¤„ç†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    print("\n" + "=" * 50)
    print("å¼€å§‹ç§»åŠ¨æ–‡ä»¶...")
    print("=" * 50)

    train_files_moved = process_files(train_names, train_dir, "è®­ç»ƒ")
    val_files_moved = process_files(val_names, val_dir, "éªŒè¯")

    # å°è¯•åˆ é™¤åŸå§‹çš„ç©ºæ–‡ä»¶å¤¹
    print("\nğŸ§¹ æ¸…ç†åŸå§‹ç›®å½•...")
    for dir_to_remove in [images_dir, labels_dir]:
        try:
            if os.path.exists(dir_to_remove) and not os.listdir(dir_to_remove):
                os.rmdir(dir_to_remove)
                print(f"âœ… å·²åˆ é™¤ç©ºç›®å½•: {dir_to_remove}")
            elif os.path.exists(dir_to_remove):
                print(f"âš ï¸  ç›®å½•éç©ºï¼Œä¿ç•™: {dir_to_remove}")
        except OSError as e:
            print(f"âŒ åˆ é™¤ç›®å½•å¤±è´¥ {dir_to_remove}: {e}")

    # æ‰“å°æœ€ç»ˆç»“æœ
    print("\n" + "=" * 50)
    print("ğŸ‰ æ•°æ®é›†åˆ†å‰²å®Œæˆï¼")
    print("=" * 50)
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"   - è®­ç»ƒé›†: {len(train_names)} å›¾åƒ")
    print(f"   - éªŒè¯é›†: {len(val_names)} å›¾åƒ")
    print(f"   - éªŒè¯é›†æ¯”ä¾‹: {val_ratio:.2f} ({len(val_names) / len(base_names) * 100:.1f}%)")
    print(f"   - éšæœºç§å­: {seed}")
    print(f"   - æ€»ç§»åŠ¨æ–‡ä»¶æ•°: {train_files_moved + val_files_moved}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•:")
    print(f"   - è®­ç»ƒé›†: {train_dir}")
    print(f"   - éªŒè¯é›†: {val_dir}")

    return train_names, val_names


def main():
    """
    å‘½ä»¤è¡Œå…¥å£å‡½æ•°
    """
    import argparse

    parser = argparse.ArgumentParser(
        description='åˆ†å‰²YOLOæ ¼å¼çš„æ•°æ®é›†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ä½¿ç”¨ç¤ºä¾‹:
  python split_data.py ./dataset                    # ä½¿ç”¨é»˜è®¤å‚æ•°
  python split_data.py ./dataset --val_ratio 0.3   # è‡ªå®šä¹‰éªŒè¯é›†æ¯”ä¾‹
  python split_data.py ./dataset --seed 123        # è®¾ç½®éšæœºç§å­
  python split_data.py ./dataset --max_workers 16  # ä½¿ç”¨æ›´å¤šçº¿ç¨‹
        '''
    )
    parser.add_argument('path', help='æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„ï¼ŒåŒ…å«imageså’Œlabelsæ–‡ä»¶å¤¹')
    parser.add_argument('--val_ratio', type=float, default=0.2,
                        help='éªŒè¯é›†æ¯”ä¾‹ï¼Œé»˜è®¤0.2')
    parser.add_argument('--seed', type=int, default=42,
                        help='éšæœºç§å­ï¼Œé»˜è®¤42')
    parser.add_argument('--max_workers', type=int, default=8,
                        help='æœ€å¤§çº¿ç¨‹æ•°ï¼Œé»˜è®¤8')

    args = parser.parse_args()

    try:
        split_yolo_dataset(
            path=args.path,
            val_ratio=args.val_ratio,
            seed=args.seed,
            max_workers=args.max_workers
        )
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        return 1

    return 0


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    dataset_path = "./fire-smoke/combined-15000"  # æ›¿æ¢ä¸ºä½ çš„æ•°æ®é›†è·¯å¾„

    # å‚æ•°é…ç½®
    val_ratio = 0.2  # éªŒè¯é›†æ¯”ä¾‹
    random_seed = 42  # éšæœºç§å­
    max_workers = 8  # æœ€å¤§çº¿ç¨‹æ•°

    try:
        train_files, val_files = split_yolo_dataset(
            path=dataset_path,
            val_ratio=val_ratio,
            seed=random_seed,
            max_workers=max_workers
        )

        print(f"\nâœ… åˆ†å‰²å®Œæˆ!")
        print(f"è®­ç»ƒé›†æ ·æœ¬: {len(train_files)} ä¸ª")
        print(f"éªŒè¯é›†æ ·æœ¬: {len(val_files)} ä¸ª")

    except Exception as e:
        print(f"âŒ æ•°æ®é›†åˆ†å‰²å¤±è´¥: {e}")